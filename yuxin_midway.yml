DSC:
  midway2:
    description: UChicago RCC cluster Midway 2
    address: {user_name}@midway2.rcc.uchicago.edu
    paths:
      home: /home/{user_name}
    queue_type: pbs
    status_check_interval: 30
    max_running_jobs: 50
    max_cores: 40
    max_walltime: "36:00:00"
    max_mem: 64G
    job_template: |
      #!/bin/bash
      #{partition}
      #{account}
      #SBATCH --time={walltime}
      #SBATCH --nodes={nodes}
      #SBATCH --cpus-per-task={cores}
      #SBATCH --mem={mem//10**9}G
      #SBATCH --job-name={job_name}
      #SBATCH --output={cur_dir}/{job_name}.out
      #SBATCH --error={cur_dir}/{job_name}.err
      cd {cur_dir}
      module load R/3.5.1 2> /dev/null
    partition: "SBATCH --partition=broadwl"
    account: ""
    submit_cmd: sbatch {job_file}
    submit_cmd_output: "Submitted batch job {job_id}"
    status_cmd: squeue --job {job_id}
    kill_cmd: scancel {job_id}
  midway2_head:
    based_on: midway2
    address: localhost
  stephenslab:
    based_on: midway2_head
    max_cores: 28
    max_mem: 128G
    max_walltime: "10d"
    partition: "SBATCH --partition=mstephens"
    account: "SBATCH --account=pi-mstephens"

default:
  queue: midway2_head
  time_per_instance: 2m
  # it means each CPU will run 120 / (4 * 10) = 3 sequential jobs
  instances_per_job: 120
  nodes_per_job: 10
  cpus_per_node: 4
  cpus_per_instance: 1
  mem_per_instance: 2G
  time_per_instance: 3m

susie_rss:
  time_per_instance: 61m

susie_rss_add_z:
  time_per_instance: 61m

susie_bhat:
  time_per_instance: 61m

susie_bhat_add_z:
  time_per_instance: 61m

finemap:
  time_per_instance: 61m

finemap_add_z:
  time_per_instance: 61m

dap_z:
  time_per_instance: 61m
  mem_per_instance: 6G
